{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spec: Core i7 8550U 4 cores 8 threads\n",
    "# Jupyter notebook on windows 11\n",
    "# python 3.9.7\n",
    "\n",
    "# DATASET\n",
    "# https://github.com/UniversalDependencies/UD_Vietnamese-VTB 1400 sentences\n",
    "\n",
    "# REQUIREMENTS\n",
    "# conll==4.4.1\n",
    "# ViSpacy==0.0.1 https://gitlab.com/trungtv/vi_spacy/-/raw/master/vi_core_news_lg/dist/vi_core_news_lg-0.0.1.tar.gz\n",
    "# Underthesea==1.3.4a0\n",
    "# Vncorenlp==1.0.3 \n",
    "# Java 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sentences: 1400\n",
      "Sentence: mảnh đất của đạn bom không còn người nghèo.\n",
      "Tag: [['mảnh', 'Nc', ' '], ['đất', 'N', ' '], ['của', 'E', ' '], ['đạn', 'N', ' '], ['bom', 'N', ' '], ['không', 'R', ' '], ['còn', 'V', ' '], ['người', 'N', ' '], ['nghèo', 'A', ' '], ['.', '.', ' ']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Format for data [[[word, pos, entity], [word, pos, entity],...]]\n",
    "with open ('data/vi_vtb-ud-train.conllu', 'r', encoding='utf-8') as f:\n",
    "\ttext = f.read()\n",
    "\n",
    "sentences = parse(text)\n",
    "\n",
    "text = ''\n",
    "sents = []\n",
    "groundtruth = []\n",
    "for tokenlist in sentences:\n",
    "    tagged = []\n",
    "    sent = tokenlist.metadata['text']\n",
    "    for item in tokenlist:\n",
    "        tagged.append([item['form'],item['xpos'],' '])\n",
    "    text += sent + ' '\n",
    "    groundtruth.append(tagged)\n",
    "    sents.append(sent)\n",
    "\n",
    "print(f\"Num sentences: {len(sents)}\")\n",
    "print(f\"Sentence: {sents[0]}\\nTag: {groundtruth[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def annotate(self, text):\n",
    "        pass\n",
    "    def tokenize(self, text):\n",
    "        pass\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Cộng đồng', 'N', ''],\n",
       " ['xử lý', 'V', ''],\n",
       " ['ngôn ngữ', 'N', ''],\n",
       " ['tự nhiên', 'A', '']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Vi_Spacy(tokenizer):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\timport spacy\n",
    "\t\tself.nlp = spacy.load('vi_core_news_lg')\n",
    "\n",
    "\tdef annotate(self,text):\n",
    "\t\toutput = []\n",
    "\t\tdoc = self.nlp(text)\n",
    "\n",
    "\t\tfor token in doc:\n",
    "\t\t    output.append([token.text.replace('_',' '), token.tag_, ''])\n",
    "\t\t    #print(token.text, token.lemma_, token.tag_, token.pos_, token.dep_,\n",
    "\t\t    #        token.shape_, token.is_alpha, token.is_stop)\n",
    "\n",
    "\t\treturn output\n",
    "\n",
    "\n",
    "\tdef info(self):\n",
    "\t\treturn('ViSpacy')\n",
    "\t\n",
    "a = Vi_Spacy()\n",
    "a.annotate(\"Cộng đồng xử lý ngôn ngữ tự nhiên\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Underthesea(tokenizer):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tfrom underthesea import word_tokenize\n",
    "\t\tfrom underthesea import ner\n",
    "\t\tself.word_tokenize = word_tokenize\n",
    "\t\tself.ner = ner\n",
    "\n",
    "\tdef annotate(self,text):\n",
    "\t\toutput = []\n",
    "\t\tners = self.ner(text)\n",
    "\t\tfor item in ners:\n",
    "\t\t\toutput.append([item[0],item[1],item[3]])\n",
    "\t\t\n",
    "\t\treturn output\n",
    "\n",
    "\tdef tokenize(self, text):\n",
    "\t\treturn self.word_tokenize(text)\n",
    "\n",
    "\tdef info(self):\n",
    "\t\treturn('Underthesea')\n",
    "\n",
    "# a = Underthesea()\n",
    "# a.annotate('Cộng đồng xử lý ngôn ngữ tự nhiên')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VncoreNLP(tokenizer):\n",
    "\t# To perform word segmentation, POS tagging, NER and then dependency parsing\n",
    "\t# annotator = VnCoreNLP(\"VnCoreNLP-1.1.1.jar\", annotators=\"wseg,pos,ner,parse\", max_heap_size='-Xmx2g')\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tfrom vncorenlp import VnCoreNLP\n",
    "\t\tself.annotator = VnCoreNLP(\"VnCoreNLP-1.1.1.jar\", annotators=\"wseg,pos,ner\", max_heap_size='-Xmx2g')\n",
    "\t\t\n",
    "\tdef annotate(self,text):\n",
    "\t\toutput = []\n",
    "\t\tannotated_text = self.annotator.annotate(text)\n",
    "\t\tfor sent in annotated_text['sentences']:\n",
    "\t\t\tfor item in sent:\n",
    "\t\t\t\toutput.append([item['form'].replace('_',' '), item['posTag'], item['nerLabel']])\n",
    "\t\treturn output\n",
    "\t\n",
    "\tdef tokenize(self, text):\n",
    "\t\tself.annotator.tokenize(text)\n",
    "\t\t\t\n",
    "\tdef info(self):\n",
    "\t\treturn('VnCoreNLP')\n",
    "\n",
    "\tdef close(self):\n",
    "\t\tself.annotator.close()\n",
    "\n",
    "# a = VncoreNLP()\n",
    "# a.annotate(\"Cộng đồng xử lý ngôn ngữ tự nhiên\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ViSpacy\n",
      "Tagging time: 0.97577s Tokenize time: 0.00764s Word segmentation acc: 0.69662 Pos tag acc: 0.00000 \n",
      "\n",
      "Underthesea\n",
      "Tagging time: 25.44487s Tokenize time: 2.81359s Word segmentation acc: 0.79635 Pos tag acc: 0.59990 \n",
      "\n",
      "VnCoreNLP\n",
      "Tagging time: 46.65205s Tokenize time: 35.96454s Word segmentation acc: 0.77491 Pos tag acc: 0.62475 \n"
     ]
    }
   ],
   "source": [
    "for t in (Vi_Spacy,Underthesea,VncoreNLP):\n",
    "    t = t()\n",
    "    count = 0\n",
    "    wordcount = 0\n",
    "    poscount = 0\n",
    "    sercount = 0\n",
    "\n",
    "\n",
    "    time_annotate = 0\n",
    "    time_tokenize = 0\n",
    "    index = 0\n",
    "\n",
    "    for sent in sents:\n",
    "\n",
    "        start = time.time()\n",
    "        predict = t.annotate(sent) # time for segmentation, postag, and ner\n",
    "        time_annotate += time.time() - start\n",
    "\n",
    "        start = time.time()\n",
    "        predict_tokenize = t.tokenize(sent)\n",
    "        time_tokenize += time.time() - start\n",
    "\n",
    "        count += len(groundtruth[index])\n",
    "\n",
    "        if len(predict) == len(groundtruth[index]): # only add to count if num predicted words equals that of groundtruth\n",
    "            for item,gt in zip(predict,groundtruth[index]):  # item = [word, pos, entity]\n",
    "                if item[0] == gt[0]:\n",
    "                    wordcount += 1\n",
    "                if item[1] == gt[1]:\n",
    "                    poscount += 1\n",
    "                if item[2] == gt[2]:\n",
    "                    sercount += 1\n",
    "        index += 1\n",
    "\n",
    "    # Corrected segmented word and entity / total word count\n",
    "    wordsegacc = wordcount/count\n",
    "    posacc = poscount/count\n",
    "    seracc = sercount/count\n",
    "\n",
    "    print()\n",
    "    print(t.info())\n",
    "    print(f\"Tagging time: {time_annotate:.5f}s Tokenize time: {time_tokenize:.5f}s Word segmentation acc: {wordsegacc:.5f} Pos tag acc: {posacc:.5f} \")\n",
    "\n",
    "    t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'CocCocTokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11292/1582760695.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mCocCocTokenizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPyTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# load_nontone_data is True by default\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPyTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_nontone_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'CocCocTokenizer'"
     ]
    }
   ],
   "source": [
    "from CocCocTokenizer import PyTokenizer\n",
    "\n",
    "# load_nontone_data is True by default\n",
    "T = PyTokenizer(load_nontone_data=True)\n",
    "\n",
    "# tokenize_option:\n",
    "# \t0: TOKENIZE_NORMAL (default)\n",
    "#\t1: TOKENIZE_HOST\n",
    "#\t2: TOKENIZE_URL\n",
    "print(T.word_tokenize(\"xin chào, tôi là người Việt Nam\", tokenize_option=0))\n",
    "\n",
    "# output: ['xin', 'chào', ',', 'tôi', 'là', 'người', 'Việt_Nam']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "380030d1298d5a27518acca789ff38fe82bbf2e68b73263de6a6bf23efb7704c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
